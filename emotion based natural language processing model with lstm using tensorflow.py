# -*- coding: utf-8 -*-
"""Emotion-based Natural Language Processing (NLP) Model with LSTM using TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MhxSYkCWF5sG3gIjOsyJLndxvzLZo9Jw

###Project Name : Emotion-based Natural Language Processing (NLP) Model with LSTM using TensorFlow
###Project Description : This project aims to implement Natural Language Processing (NLP) using TensorFlow with a focus on emotion datasets. It utilizes a sequential model architecture that integrates LSTM layers to understand and process text containing emotions. The main goal is to improve the understanding of emotional context in text, both for sentiment classification and emotion analysis.
###Dataset : Emotion dataset
###Tools : Google Colaboratory

Import
"""

import pandas as pd
df = pd.read_csv('Emotion_final.csv')

df.head()

"""one-hot-encoding dan dataframe baru"""

category = pd.get_dummies(df.Emotion)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='Emotion')
df_new

"""Tipe Data Numpy Array"""

tulisan = df_new['Text'].values
label = df_new[['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']].values

tulisan

label

"""Training dan Testing Data"""

from sklearn.model_selection import train_test_split
tulisan_latih, tulisan_test, label_latih, label_test = train_test_split(tulisan, label, test_size=0.2)

"""Tokenizer dan konversi menjadi sequence"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(tulisan_latih)
tokenizer.fit_on_texts(tulisan_test)

sekuens_latih = tokenizer.texts_to_sequences(tulisan_latih)
sekuens_test = tokenizer.texts_to_sequences(tulisan_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""Embedding"""

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])

model.summary()

"""Callbacks"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc') > 0.9):
      print("\nAkurasi lebih dari 90%")
      self.model.stop_trainging = True

callbacks = myCallback()

"""Fit model"""

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=1, validation_steps=25,
callbacks=[callbacks])

"""Model Accuracy"""

from matplotlib import pyplot as plt
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Accuracy Plot')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""Model Loss"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Plot')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()